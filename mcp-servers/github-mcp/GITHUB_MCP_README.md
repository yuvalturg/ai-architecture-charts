# GitHub MCP (Model Control Plane) README


Welcome to the **GitHub MCP (Model Control Plane)** server! This server serves as the backbone for managing, deploying, and monitoring your AI/ML models on GitHub-based infrastructures. This README will guide you through understanding, setting up, and leveraging the GitHub MCP in your workflow.

---

## ğŸ“š Overview

- **Purpose:** Centralizes control of AI/ML models, automating deployments, scaling, and monitoring through GitHub.
- **Scope:** Ideal for teams or projects needing robust lifecycle management of models with tight DevOps integration.

---

## ğŸš€ Features

- We have functionality for the following github functions:

Here are the main GitHub MCP functions organized by category:

### ğŸ“ Issues & Comments
- `add_issue_comment` â€” Add a comment to an issue
- `assign_copilot_to_issue` â€” Assign Copilot to an issue
- `issue_read` â€” Read issue details
- `issue_write` â€” Write/update an issue
- `sub_issue_write` â€” Create/update a sub-issue

### ğŸ” Search
- `search_code` â€” Search code in repositories
- `search_issues` â€” Search for issues
- `search_pull_requests` â€” Search pull requests
- `search_repositories` â€” Search repositories
- `search_users` â€” Search for users

### ğŸŒ¿ Branches, Commits & Tags
- `create_branch` â€” Create a new branch
- `list_branches` â€” List all branches
- `list_commits` â€” List repository commits
- `get_commit` â€” Get a particular commit
- `get_tag` â€” Get tag details
- `list_tags` â€” List all tags

### ğŸ“‚ Files & Repositories
- `create_or_update_file` â€” Create or update a file
- `delete_file` â€” Delete a file
- `get_file_contents` â€” Retrieve contents of a file
- `push_files` â€” Push files to the repository
- `create_repository` â€” Create a new repository
- `fork_repository` â€” Fork an existing repository

### ğŸ“¦ Releases
- `get_latest_release` â€” Get the latest release
- `get_release_by_tag` â€” Get a release by tag
- `list_releases` â€” List all releases

### ğŸ”– Labels & Teams
- `get_label` â€” Get a label
- `get_teams` â€” List teams
- `get_team_members` â€” List team members

### ğŸ‘¤ User Info
- `get_me` â€” Get user credentials/info

### ğŸ”€ Pull Requests
- `create_pull_request` â€” Create a pull request
- `list_pull_requests` â€” List pull requests
- `update_pull_request` â€” Update a pull request
- `merge_pull_request` â€” Merge a pull request
- `update_pull_request_branch` â€” Update PR branch from base
- `pull_request_read` â€” Read pull request details
- `pull_request_review_write` â€” Submit or edit a PR review
- `add_comment_to_pending_review` â€” Add comment to a pending PR review
- `request_copilot_review` â€” Request Copilot review for a PR

### â„¹ï¸ Other
- `list_issue_types` â€” List all issue types


---

## ğŸ“¦ Getting Started

### Prerequisites

- [Node.js](https://nodejs.org/) or [Python](https://www.python.org/) (depending on the implementation)
- [Docker](https://www.docker.com/get-started) (optional, recommended)
- GitHub account with sufficient repository permissions
- (Optional) Access to cloud platforms (e.g., AWS, GCP, Azure) for deployment integrations

### Installation

#### 1. Clone the repository

```bash
git clone https://github.com/YOUR_ORG/github-mcp.git
cd github-mcp
```

#### 2. Install dependencies

For Node.js:
```bash
npm install
```
For Python:
```bash
pip install -r requirements.txt
```

#### 3. Set up environment variables

Copy `.env.example` to `.env` and edit as needed.

#### 4. Start the server

```bash
npm start
```
or  
```bash
python app.py
```

---

## ğŸ› ï¸ Deployment

- **Required:**
    - User must set GITHUB_PERSONAL_ACCESS_TOKEN in order to use this tool.

All the MCP Servers are running in similar ways. 

The MCP (Multi-Cloud Platform) servers are designed to run as simple, stateless web services that can be deployed on any standard infrastructureâ€”locally, on VMs, or in containers. Each server listens for HTTP requests and exposes endpoints for processing GitHub metadata, repository events, or user-specified automations.

**How MCP Servers Run:**

1. **Start via CLI or Docker:**  
   MCP servers can be started with `npm start`, `python app.py`, or via a Docker container, depending on your setup.

2. **Environment Configuration:**  
   Critical configuration, such as GitHub tokens, endpoint URLs, and cloud provider credentials, are provided through environment variables (see `.env.example`). This allows secure and flexible deployment.

3. **Stateless Operation:**  
   Servers do not persist data internally; instead, they interact with GitHub APIs and any configured cloud backends in real-time. State is managed either externally (e.g., via GitHub Issues, metadata in cloud storage, etc.) or passed per request.

4. **Modular Design:**  
   Each MCP server runs independently and has a modular, plug-in-based backend integration model. You can enable or disable cloud and CI/CD integrations via config.

5. **Secure Web Server:**  
   The server exposes a secured HTTP API, typically protected with GitHub OAuth or personal access token validation. No requests are processed without proper authentication.

6. **Logs and Monitoring:**  
   All runtime logs are output to the console or can be redirected to logging systems or file storage, depending on deployment standards.

**Example:**

- For local runs, after installing dependencies and configuring `.env`, run `npm start` (Node.js) or `python app.py` (Python).
- For Docker deployments:  
  - Build the image (e.g., `docker build -t github-mcp .`)
  - Run with environment variables:  
    `docker run -p 8080:8080 --env-file .env github-mcp`

Once started, the server will listen for HTTP requests and handle events as configured.

**Note**

In order to get the correct output for the LLM when uysing LLamastack be sure to enable tool outputs. 
For example for `LLM=llama-3-2-3b-instruct` you must set:

```yaml
args:
  - --enable-auto-tool-choice          # Enables automatic tool selection
  - --chat-template                     # Specifies template path
  - /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja
  - --tool-call-parser                  # Parser for Llama 3.2 JSON format
  - llama3_json
  - --max-model-len                     # Max context length
  - "30444"
```

## Building

```bash
podman build --no-cache --platform linux/amd64 \
  -t quay.io/rh-ai-quickstart/github-mcp:0.5.7 \
  -f mcp-servers/github-mcp/Containerfile \
  mcp-servers/github-mcp/
```