device: gpu

servingRuntime:
  name: vllm-serving-runtime
  knativeTimeout: 60m
  gpuImage: quay.io/ecosystem-appeng/vllm:openai-v0.8.5
  cpuImage: quay.io/ecosystem-appeng/vllm:cpu-v0.8.5
  recommendedAccelerators:
    - nvidia.com/gpu
  env:
    - name: HOME
      value: /vllm
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          key: HF_TOKEN
          name: huggingface-secret
  volumeMounts:
    - name: shm
      mountPath: /dev/shm
    - name: vllm-home
      mountPath: /vllm
    - name: vllm-chat-templates
      mountPath: /chat-templates
  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi
    - name: vllm-home
      emptyDir:
        sizeLimit: 5Gi
    - name: vllm-chat-templates
      configMap:
        name: vllm-chat-templates

secret:
  enabled: true
  hf_token: ""

models:
  llama-3-2-1b-instruct:
    id: meta-llama/Llama-3.2-1B-Instruct
    enabled: false
    args:
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "30544"

  llama-guard-3-1b:
    id: meta-llama/Llama-Guard-3-1B
    enabled: false
    args:
      - --max-model-len
      - "14336"

  llama-3-2-3b-instruct:
    id: meta-llama/Llama-3.2-3B-Instruct
    enabled: false
    args:
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "30544"

  llama-guard-3-8b:
    id: meta-llama/Llama-Guard-3-8B
    enabled: false
    args:
      - --max-model-len
      - "14336"

  llama-3-1-8b-instruct:
    id: meta-llama/Llama-3.1-8B-Instruct
    enabled: false
    resources:
      limits:
        nvidia.com/gpu: "1"
    args:
      - --max-model-len
      - "14336"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json

  llama-3-3-70b-instruct:
    id: meta-llama/Llama-3.3-70B-Instruct
    enabled: false
    storageSize: 150Gi
    resources:
      limits:
        nvidia.com/gpu: "4"
    args:
      - --tensor-parallel-size
      - "4"
      - --gpu-memory-utilization
      - "0.95"
      - --quantization
      - fp8
      - --max-num-batched-tokens
      - "4096"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --swap-space
      - "32"

  llama-3-2-1b-instruct-quantized:
    id: RedHatAI/Llama-3.2-1B-Instruct-quantized.w8a8
    enabled: false
    args:
      - --gpu-memory-utilization
      - "0.4"
      - --quantization
      - compressed-tensors
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "30544"
