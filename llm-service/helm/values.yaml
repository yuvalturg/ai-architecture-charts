device: gpu  # Default device for models that don't specify one
rawDeploymentMode: true

# Device-specific configurations for tolerations and accelerators
deviceConfigs:
  gpu:  # NVIDIA GPU configuration
    image: vllm/vllm-openai:v0.11.1
    tolerations:
      - key: nvidia.com/gpu
        effect: NoSchedule
        operator: Exists
    recommendedAccelerators:
      - nvidia.com/gpu
    acceleratorType: nvidia.com/gpu
  gpu-nightly-vllm:
    image: vllm/vllm-openai:nightly
    tolerations:
      - key: nvidia.com/gpu
        effect: NoSchedule
        operator: Exists
    recommendedAccelerators:
      - nvidia.com/gpu
    acceleratorType: nvidia.com/gpu
  hpu:
    image: quay.io/modh/vllm:rhoai-2.25-gaudi
    tolerations:
      - key: habana.ai/gaudi
        effect: NoSchedule
        operator: Exists
    recommendedAccelerators:
      - habana.ai/gaudi
    acceleratorType: habana.ai/gaudi
  gpu-amd:  # AMD GPU configuration (ROCm)
    image: quay.io/modh/vllm:rhoai-2.25-rocm  # RHOAI 2.25 with ROCm support
    tolerations:
      - key: amd.com/gpu
        effect: NoSchedule
        operator: Exists
    recommendedAccelerators:
      - amd.com/gpu
    acceleratorType: amd.com/gpu
  cpu:
    image: quay.io/ecosystem-appeng/vllm:cpu-v0.9.2
    tolerations: []
    recommendedAccelerators: []
    acceleratorType: null
  xeon:  # Intel Xeon CPU configuration for optimized performance, Sapphire Rapids and later
    image: opea/vllm-cpu-ubi:v0.12.0-ubi9
    tolerations: []
    recommendedAccelerators: []
    acceleratorType: null
    # Resource settings for Xeon deployments
    # Ignores model-specific cpu and memory settings and uses these instead for all models on xeon for better performance
    # Minimum recommended for inference: 16 vCPU and 64Gi memory
    resources:
      limits:
        cpu: "32"
        memory: 64Gi
      requests:
        cpu: "16"
        memory: 64Gi

servingRuntime:
  name: vllm-serving-runtime
  knativeTimeout: 60m
  env:
    - name: HOME
      value: /vllm
    - name: HF_HUB_OFFLINE
      value: "0"
    - name: VLLM_SKIP_WARMUP
      value: "true"
    - name: HF_TOKEN
      valueFrom:
        secretKeyRef:
          key: HF_TOKEN
          name: huggingface-secret
  volumeMounts:
    - name: shm
      mountPath: /dev/shm
    - name: vllm-chat-templates
      mountPath: /chat-templates
    - name: vllm-home
      mountPath: /vllm
  volumes:
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi
    - name: vllm-chat-templates
      configMap:
        name: vllm-chat-templates
    - name: vllm-home
      emptyDir: {}

secret:
  enabled: true
  hf_token: ""

models:
  llama-3-2-1b-instruct:
    id: meta-llama/Llama-3.2-1B-Instruct
    enabled: false
    # device: hpu  # Can be gpu, gpu-amd, hpu, or cpu - defaults to global device if not specified
    # A default "nvidia.com/gpu" toleration is implemented in the
    # inference-service.yaml template and can be overriden as follows:
    # tolerations:
    #   - key: nvidia.com/gpu
    #     effect: NoSchedule
    #     operator: Exists
    resources:
      limits:
        cpu: "2"
        memory: 32Gi
      requests:
        cpu: "1"
        memory: 16Gi
    args:
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"

  llama-guard-3-1b:
    id: meta-llama/Llama-Guard-3-1B
    enabled: false
    resources:
      limits:
        cpu: "2"
        memory: 32Gi
      requests:
        cpu: "1"
        memory: 16Gi
    args:
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"

  llama-3-2-3b-instruct:
    id: meta-llama/Llama-3.2-3B-Instruct
    enabled: false
    resources:
      limits:
        cpu: "2"
        memory: 32Gi
      requests:
        cpu: "1"
        memory: 16Gi
    args:
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"

  llama-guard-3-8b:
    id: meta-llama/Llama-Guard-3-8B
    enabled: false
    resources:
      limits:
        cpu: "2"
        memory: 32Gi
      requests:
        cpu: "1"
        memory: 16Gi
    args:
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"

  llama-3-1-8b-instruct:
    id: meta-llama/Llama-3.1-8B-Instruct
    enabled: false
    resources:
      limits:
        cpu: "2"
        memory: 32Gi
      requests:
        cpu: "1"
        memory: 16Gi
    args:
      - --max-model-len
      - "14336"
      - --max-num-seqs
      - "32"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json

  llama-3-3-70b-instruct:
    id: meta-llama/Llama-3.3-70B-Instruct
    enabled: false
    storageSize: 150Gi
    accelerators: "4"  # Number of accelerators needed different than 1
    resources:
      limits:
        cpu: "2"
        memory: 300Gi
      requests:
        cpu: "1"
        memory: 200Gi
    args:
      - --tensor-parallel-size
      - "4"
      - --gpu-memory-utilization
      - "0.95"
      - --max-num-batched-tokens
      - "131074"
      - --max-num-seqs
      - "32"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --swap-space
      - "32"

  llama-3-3-70b-instruct-quantization-fp8:
    id: meta-llama/Llama-3.3-70B-Instruct
    enabled: false
    storageSize: 150Gi
    accelerators: "4"
    args:
      - --tensor-parallel-size
      - "4"
      - --gpu-memory-utilization
      - "0.95"
      - --quantization
      - fp8
      - --max-num-batched-tokens
      - "4096"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --swap-space
      - "32"

  llama-3-2-1b-instruct-quantized:
    id: RedHatAI/Llama-3.2-1B-Instruct-quantized.w8a8
    enabled: false
    args:
      - --gpu-memory-utilization
      - "0.4"
      - --quantization
      - compressed-tensors
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_llama3.2_json.jinja
      - --tool-call-parser
      - llama3_json
      - --max-model-len
      - "30544"

  gpt-oss-20b:
    id: openai/gpt-oss-20b
    enabled: false
    storageSize: 80Gi
    args:
      - --tool-call-parser
      - openai
      - --enable-auto-tool-choice

  qwen-2-5-vl-3b-instruct:
    id: Qwen/Qwen2.5-VL-3B-Instruct
    args:
      - --max-model-len
      - "30544"
      - --enable-auto-tool-choice
      - --chat-template
      - /chat-templates/tool_chat_template_qwen.jinja
      - --tool-call-parser
      - llama3_json
